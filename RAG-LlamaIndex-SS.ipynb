{"cells":[{"attachments":{},"cell_type":"markdown","id":"b8daf6a9-a586-4518-94a6-26d9f84e02e9","metadata":{"language":"python"},"source":"# RAG with LlamaIndex"},{"attachments":{},"cell_type":"markdown","id":"3cc75faf-1b26-4f50-9d4f-dd50bdc1f4da","metadata":{"language":"python"},"source":"![RAG LlamaIndex](https://media.licdn.com/dms/image/v2/D5622AQHrFw7nRO7GFg/feedshare-shrink_800/feedshare-shrink_800/0/1732033489014?e=1735171200&v=beta&t=gy1fBYDQHck9vsDld3EUjoVbJM8bJ_MXlCbiuF55Erk)"},{"cell_type":"code","execution_count":6,"id":"2f86a2ab-a1c3-4165-b24d-c490cb47891c","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:10:55.186168Z","iopub.status.busy":"2024-11-21T05:10:55.185488Z","iopub.status.idle":"2024-11-21T05:11:27.013184Z","shell.execute_reply":"2024-11-21T05:11:27.010707Z","shell.execute_reply.started":"2024-11-21T05:10:55.186124Z"},"language":"python","trusted":true},"outputs":[],"source":"!pip install -q llama-index==0.10.57 llama-index-llms-gemini==0.1.11 openai==1.37.0 google-generativeai==0.5.4"},{"cell_type":"code","execution_count":7,"id":"0456c6db-c75e-401a-a819-173fb6108bce","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:11:40.646961Z","iopub.status.busy":"2024-11-21T05:11:40.623959Z","iopub.status.idle":"2024-11-21T05:11:40.659237Z","shell.execute_reply":"2024-11-21T05:11:40.658075Z","shell.execute_reply.started":"2024-11-21T05:11:40.646924Z"},"language":"python","trusted":true},"outputs":[],"source":"import os\n\n# Set the following API Keys in the Python environment. Will be used later.\nos.environ[\"OPENAI_API_KEY\"] = \"Add your OpenAI API Key\"\nos.environ[\"GOOGLE_API_KEY\"] = \"Add your Google API Key\""},{"cell_type":"code","execution_count":31,"id":"3f711e4a-7f02-44d5-94fb-0c193bb293de","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:20:05.454395Z","iopub.status.busy":"2024-11-21T05:20:05.451991Z","iopub.status.idle":"2024-11-21T05:20:08.591731Z","shell.execute_reply":"2024-11-21T05:20:08.590112Z","shell.execute_reply.started":"2024-11-21T05:20:05.454315Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 20329  100 20329    0     0  88431      0 --:--:-- --:--:-- --:--:-- 93682\n"}],"source":"!curl -o ./mani-dataset.csv https://raw.githubusercontent.com/pavanbelagatti/LlamaIndex-RAG-Demo/refs/heads/main/Articles-SS.csv"},{"cell_type":"code","execution_count":32,"id":"82f06546-30ab-4bb4-ae0a-c08fa91d921e","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:20:17.934585Z","iopub.status.busy":"2024-11-21T05:20:17.932604Z","iopub.status.idle":"2024-11-21T05:20:17.963739Z","shell.execute_reply":"2024-11-21T05:20:17.962311Z","shell.execute_reply.started":"2024-11-21T05:20:17.934490Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"number of articles: 9\n"}],"source":"import csv\n\nrows = []\n\n# Load the CSV file\nwith open(\"./mani-dataset.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n    csv_reader = csv.reader(file)\n\n    for idx, row in enumerate(csv_reader):\n        if idx == 0:\n            continue\n            # Skip header row\n        rows.append(row)\n\n# The number of characters in the dataset.\nprint(\"number of articles:\", len(rows))"},{"cell_type":"code","execution_count":33,"id":"c1f51e5e-5f31-42dc-a29b-15fbca2ce0da","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:20:23.136738Z","iopub.status.busy":"2024-11-21T05:20:23.136239Z","iopub.status.idle":"2024-11-21T05:20:23.197279Z","shell.execute_reply":"2024-11-21T05:20:23.195704Z","shell.execute_reply.started":"2024-11-21T05:20:23.136708Z"},"language":"python","trusted":true},"outputs":[],"source":"from llama_index.core import Document\n\n# Convert the texts to Document objects so the LlamaIndex framework can process them.\ndocuments = [Document(text=row[1]) for row in rows]"},{"cell_type":"code","execution_count":34,"id":"0fe98bd1-524c-4c2a-9ec7-1a1053da7921","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:20:26.490436Z","iopub.status.busy":"2024-11-21T05:20:26.489998Z","iopub.status.idle":"2024-11-21T05:20:26.496648Z","shell.execute_reply":"2024-11-21T05:20:26.495935Z","shell.execute_reply.started":"2024-11-21T05:20:26.490394Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"Document(id_='502a18e5-7503-4b81-a7e4-4aadb3b75616', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='In the world of generative AI, that’s Amazon Web Services (AWS) and SingleStore.\\n\\nAs gen AI progresses from novelty to necessity, SingleStore and AWS have jointly committed to accelerating possibilities for gen AI at the enterprise level. SingleStore’s latest AWS gGen AI agreement supports machine learning and generative AI initiatives with tailored resources, funding and shared expertise.\\n\\nThe partnership is highly strategic, given our complementary strengths and vision.   \\n\\nAs the largest and most widely used cloud computing platform, AWS is a natural choice for creating AI agents that function autonomously and can be orchestrated into deterministic workflows with predictable outcomes. Such orchestration is pivotal for ensuring reliability and efficiency in AI-driven processes, and is part of what makes AWS a platform of choice for developers in enterprises.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":"documents[0]"},{"cell_type":"code","execution_count":35,"id":"3f3c3eec-add1-43bc-9dac-634d4720303b","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:20:31.865062Z","iopub.status.busy":"2024-11-21T05:20:31.864703Z","iopub.status.idle":"2024-11-21T05:20:32.673212Z","shell.execute_reply":"2024-11-21T05:20:32.671437Z","shell.execute_reply.started":"2024-11-21T05:20:31.865036Z"},"language":"python","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccc3eb8156f04e6d8cc65d001066fed3","version_major":2,"version_minor":0},"text/plain":"Parsing nodes:   0%|          | 0/9 [00:00<?, ?it/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1556b5957f6b4b3eb44f6db3d24ae376","version_major":2,"version_minor":0},"text/plain":"Generating embeddings:   0%|          | 0/11 [00:00<?, ?it/s]"},"metadata":{},"output_type":"display_data"}],"source":"from llama_index.core import VectorStoreIndex\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n\n# Build index / generate embeddings using OpenAI embedding model\nindex = VectorStoreIndex.from_documents(\n    documents,\n    transformations=[SentenceSplitter(chunk_size=768, chunk_overlap=64)],\n    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n    show_progress=True,\n)"},{"cell_type":"code","execution_count":36,"id":"5b9e2dc7-6e4a-47b9-a672-9cfca12c031d","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:20:38.156865Z","iopub.status.busy":"2024-11-21T05:20:38.155870Z","iopub.status.idle":"2024-11-21T05:20:38.171252Z","shell.execute_reply":"2024-11-21T05:20:38.170537Z","shell.execute_reply.started":"2024-11-21T05:20:38.156829Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"In the world of generative AI, that’s Amazon Web Services (AWS) and SingleStore.\n\nAs gen AI progresses from novelty to necessity, SingleStore and AWS have jointly committed to accelerating possibilities for gen AI at the enterprise level. SingleStore’s latest AWS gGen AI agreement supports machine learning and generative AI initiatives with tailored resources, funding and shared expertise.\n\nThe partnership is highly strategic, given our complementary strengths and vision.   \n\nAs the largest and most widely used cloud computing platform, AWS is a natural choice for creating AI agents that function autonomously and can be orchestrated into deterministic workflows with predictable outcomes. Such orchestration is pivotal for ensuring reliability and efficiency in AI-driven processes, and is part of what makes AWS a platform of choice for developers in enterprises.\n-_-_-_-_-_-_-_-_\nHow can you tell if a software or technology product is truly loved by its users?\n\n\nAnswering that question is the mission of TrustRadius, a platform that helps customers make informed purchasing decisions based on real user experiences, in-depth reviews and peer recommendations. Validation like that can’t be bought; it must be earned.\n\n\nBut the seal of approval doesn’t stop there. Each year, TrustRadius scans reviews from real customers to find the best of the best across three pillars: capabilities, value for price and customer relationships. The products that stand out receive a Buyer’s Choice Award — an honor SingleStore is proud to have earned for 2025.\n\n\nWhile TrustRadius has previously named SingleStore top rated in categories like Relational Databases, Database as a Service, Operational Analytics and In-Memory Databases, 2024 is our first time winning for Vector Databases and our first time winning the Buyer’s Choice Award.\n\n\nThe Buyer’s Choice designation indicates strong customer satisfaction and aligns with positive metrics like a high Net Promoter Score (NPS) or low churn rate. The winning products don’t just meet technical requirements — they also provide a great user experience and ROI.\n\n\nWhat’s more, the honor is based entirely on the voices of the customers. For SingleStore, that means users like Nir Levy, Vice President of R&D, Dev Ops and Products at Zoomd Technologies.\n-_-_-_-_-_-_-_-_\nEnter the data lakehouse, an approach that combines the best features of data lakes and data warehouses to offer faster data processing and more advanced analysis.\n\n\nAs a unified platform for managing all types of data, lakehouse architecture has become increasingly important, and Apache Iceberg is a crucial part of the system because it helps manage large datasets more effectively. Companies looking to get the most out of their data find that adding real-time analytics and AI into data lakehouses can make a big impact. In this article, we’ll explore what makes data lakehouses so powerful, the role of Apache Iceberg in their success and how real-time analytics are changing the way we manage data today.\n\n\nLet’s start by understanding the challenges of traditional data storage systems, and how these systems have evolved over time.\n-_-_-_-_-_-_-_-_\nTo make SingleStore even more accessible for JavaScript and Node.js developers, we're excited to introduce a powerful new tool for developers: the @singlestore/client NPM package. By using this package within your Node.js applications, you can simplify the integration of SingleStore's high-performance capabilities into your JavaScript applications. \n\n\nUsing this package lets you quickly have complete control over your data, making real-time applications more accessible to build and maintain. With a clean and intuitive API, @singlestore/client allows you to effortlessly manage organizations, workspace groups, workspaces, databases, tables, columns, scheduled jobs and other critical components of the SingleStore Management API. This gives developers a more effortless way to leverage the full potential of SingleStore in their projects. To explore the complete list of features, visit the NPM page — which includes a full table of contents for the package's functionality.\n-_-_-_-_-_-_-_-_\nThis session highlighted how the integration of Snowflake and SingleStore creates new possibilities for real-time data analysis:\n\nReal-time data processing with SingleStore. Bharat demonstrated how SingleStore’s ability to handle high concurrency transactions with low latency enhances data analysis. By processing data directly as it lands, SingleStore significantly reduces the time required to analyze new information, achieving a 100x speed improvement in some cases.\nSeamless integration with Snowpark Container Services (SPCS). A major highlight was the introduction of SingleStore’s native app within SPCS. Now users can run a SingleStore cluster directly inside Snowflake, enabling real-time data ingestion and analysis without data ever leaving the Snowflake environment. This integration ensures a secure, unified data ecosystem with minimal latency.\nPractical use case. The demo used a ride-sharing simulation to show how SingleStore and Snowflake are better together. By ingesting real-time trip data into SingleStore through Kafka and using Snowflake for historical data analysis, the demo illustrated how companies can gain instant insights — like real-time pricing recommendations — using a combination of current and historical data.\nEnhanced data governance and security. Both speakers emphasized the importance of maintaining data security while integrating these platforms. With Snowflake's powerful governance features and SingleStore’s seamless integration capabilities, enterprises can securely manage data without sacrificing performance.\nCost efficiency and scalability. The integration enables a zero-ETL workflow, which eliminates the need for extensive data movement and minimizes the use of Snowflake credits. This results in a more cost-effective solution for enterprises looking to process large volumes of data quickly.\n-_-_-_-_-_-_-_-_\nIntroduction to knowledge graphs\nKnowledge graphs are a powerful tool for organizing and retrieving complex information. They are beneficial in RAG, which can significantly enhance the performance of LLMs. A knowledge graph is a graph structure that represents relationships between entities, which can be documents, concepts, or other data types.\n\n\nBy storing information in a graph format, knowledge graphs provide a more intuitive and flexible way to model complex, real-world scenarios. This structured approach allows for a deeper understanding of the connections and context within the data, making it easier to retrieve and utilize relevant information effectively.\n\n\nWhat is RAG?\nRAG is an approach that leverages external data stored in a database to respond to the user’s query. This enhances the quality of response generation with more context. RAG utilizes both retrieval techniques and generative models to produce contextually relevant responses. RAG improves the performance of LLMs for various natural language processing tasks, including information extraction and sentiment analysis.\n\n\n\nConsider a scenario where you would like to get custom responses from your AI application. First, the organization’s relevant documents are converted into embeddings through an embedding model and stored in a vector database. When a query is sent to the AI application, it gets converted into a vector query embedding. It goes through the vector database to find the most similar object by vector similarity search. This way, your LLM-powered application doesn’t hallucinate since you have already instructed it to ground its responses with the custom data.\n\n\nOne simple use case would be the customer support application, where the custom data is fed to the application stored in a vector database. When a user query comes in, the application generates the most appropriate response related to your products or services — not some generic answer.\n\n\nThe RAG pipeline involves three critical components: Retrieval, augmentation and generation.\n\nRetrieval. This component helps fetch relevant information from an external knowledge base, like a vector database, for any user query. It is crucial as this is the first step in curating meaningful and contextually correct responses.\nAugmentation. This part involves enhancing and adding more relevant context to the retrieved response for the user query.\nGeneration. Lastly, a final output is presented to the user with the help of an LLM. The LLM uses its knowledge and the provided context to provide an apt response to the user’s query.\nThese three components are the basis of a RAG pipeline, which helps users get the contextually rich and accurate responses they seek. That is why RAG is helpful in building chatbots, question-answering systems, etc.\n\n\nWhat are knowledge graphs in RAG?\nKnowledge graphs are structured ways to organize information, showing how entities are connected. They are used to understand relationships and context among different data points effectively. Let’s look at a simple example:\n\n\nParagraph: “Cows and dogs are good examples of animals. Cows eat herbs, which are plants! Both plants and animals count as living things.”\n\n\nFrom this unstructured text data, we can extract the following entities: cows, dogs, animals, herbs, plants and living things.\nRelationships:\n\nCows and dogs are animals\nCows eat herbs\nHerbs are plants\nPlants and animals are living things\nWith this information, we can build the following knowledge graph:\n\n\n\nKnowledge graphs significantly augment RAG systems by providing a structured semantic context that improves data retrieval accuracy and efficiency. They enable the system to understand and utilize the relationships and attributes of entities within the graph, leading to more nuanced and detailed responses. This integration enhances the functionality of RAG systems and applications and ensures the responses remain relevant over time.\n-_-_-_-_-_-_-_-_\nBuilding a knowledge graph involves integrating data from various sources to create a structured representation of knowledge. Tools and techniques like RDF, OWL and graph databases are commonly used. Vector databases can also enhance the RAG process by capturing semantic meanings and relationships.\n\n\nLLMs can create knowledge graphs — fundamentally, this is what an LLM is built to do. LLMs are constructed to understand the text and determine the important things in that text. So, it knows what entities are present and what their semantic meanings are. It also knows the relationship between those entities.\n\n\nOnce you have a knowledge graph, you can use it to perform RAG. You can do the RAG without even having vectors or vector embeddings. This approach of having knowledge graphs is suitable for handling questions about things like aggregations and multi-hop relationships. We now see a trend of new specialty databases claiming they help store the graph data and do a better RAG. But is that true? What if we tell you that your SQL databases can store and query graphs, too (using the capabilities of JOINS, Recursive CTEs, etc.)? We are going to show exactly that in this article.\n-_-_-_-_-_-_-_-_\nWhen it comes to database solutions, MariaDB and MySQL stand out as two popular choices for developers and businesses. Although these relational database management systems share a common ancestry, they have evolved to meet different needs and preferences. Understanding the key differences between MariaDB and MySQL is critical for developers, database engineers, and organizations looking to make informed decisions about their data infrastructure.\nThis article delves into the performance and feature differences between MariaDB and MySQL. It explores their development paths, compares performance metrics, and highlights the unique features that set them apart. The comparison also sheds light on the ongoing debate of MariaDB vs. MySQL performance, providing a comprehensive look at how these two database systems stack up against each other in various scenarios. Before we dig into the specifics, let's look at the foundational elements of an RDBMS.\n\nIntroduction to Relational Database Management Systems (RDBMS)\nRegarding databases and data management, the ability to efficiently store, organize, and access information is the most critical factor on which everything else is based. This is where Relational Database Management Systems (RDBMS) step in as one of the most mature and common paradigms within databases, offering a structured and systematic approach to handling vast volumes of data.\n\nWhat is a Relational Database Management System?\nAt its core, an RDBMS is database software that manages data within a tabular framework, eliminating data redundancy. At a high level, this type of database solution can be thought of as a more complex and organized spreadsheet, where information is neatly categorized into rows and columns. This tabular structure enhances data integrity and facilitates seamless querying and manipulation.\n\nRDBMS solutions leverage the Structured Query Language, more commonly referred to as SQL, a powerful tool for storing, managing, and modifying data. SQL acts as a bridge between users and the database, allowing for executing complex queries and operations with a familiar and easy-to-use syntax.\n\nWithin the database market, there are plenty of RDBMS options available. If you've landed on this article, you likely already know that MySQL and MariaDB are two trendy choices within this category, particularly in web application development. These robust platforms have repeatedly proven their mettle, offering a blend of performance, scalability, and user-friendliness. Let's take a deeper look at the history of these two within the RDBMS landscape.\n\nBrief History of MySQL and MariaDB\nMySQL's story began in 1995 when MySQL AB, a Swedish company, first released it. Since then, it has become one of the most widely used open-source RDBMS solutions, powering countless websites and applications worldwide.\n\nIn 2009, a new chapter unfolded with the birth of MariaDB. It was forked from MySQL by Michael \"Monty\" Widenius, one of the founders of MySQL AB. This new venture was driven by a vision to create a drop-in replacement for MySQL, incorporating additional features and performance enhancements while remaining true to its open-source roots.\n\nMariaDB continues to evolve, carving its own path in the RDBMS landscape. It is widely recognized for its commitment to community-driven development, emphasis on performance optimization, and compatibility with MySQL, making it an attractive option for those seeking a seamless transition or a feature-rich alternative. Recently, however, MariaDB has encountered some problems with its business finances, resulting in a takeover by private equity.\n\nOngoing troubles for MariaDB\nAs of 2024, MariaDB is facing significant challenges. After a poorly received SPAC IPO in 2022, its stock plunged over 90%, leading to two rounds of layoffs in 2023.\n-_-_-_-_-_-_-_-_\nOngoing troubles for MariaDB\nAs of 2024, MariaDB is facing significant challenges. After a poorly received SPAC IPO in 2022, its stock plunged over 90%, leading to two rounds of layoffs in 2023. Essential products, Xpand and SkySQL, were discontinued, with SkySQL returning as an independent company. Financial troubles have strained its relationship with the MariaDB foundation, while Microsoft has dropped MariaDB as a managed service in favor of MySQL. Most recently, news broke that  K1 Investment Management acquired MariaDB — taking the company back to private after being public.\n\nThese uncertainties have led to a significant setback for MariaDB in the database market as it struggles to regain stability after going private. Andy Pavlo, a professor from Carnegie Mellon University, wrote a complete breakdown of the database market in 2023 and touched on MariaDB's catastrophic ups and downs.\n-_-_-_-_-_-_-_-_\nSingleStore’s real-time digital marketing demo combines both OLTP and OLAP workloads, and is a great example of how to build real-time hybrid transactional and analytical processing (HTAP) applications on top of SingleStore.\n\nHow We Built Our Real-Time Digital Marketing App\nIn this blog, we’ll provide a high-level overview of the app, diving deeper into details  around the key features in SingleStore that make building this type of application possible — with one database.\n\n\nApp overview\nThis demo simulates a fictional telco with millions of subscribers, serving targeted offers to customers in real time. Targeted advertisements are given based on demographics, geo location, purchase history and much more. SingleStore is evaluating 80 million ad-serving opportunities,  simulating 30,000 to 50,000 ads and offers — all in real time — to cell phones. This digital marketing app sends out notifications of offers based on matching algorithms and complex customer segmentation on real-time and batch data.\n\n\n\nAdditionally, there is an operational analytics side of the application showing real-time conversion metrics for advertisers, among other statistics. The following is a picture of one of the dashboards presented in the app.\n\n\n\nBeing able to handle both OLTP and OLAP workloads in one database with a variety of different data types is one of SingleStore’s key features. Let’s take a look at how this is done within SingleStore by looking at the key features used to power this application.\n\n\nSchema design\nSingleStore has three different table tables with differing strengths that can be used to optimize performance for HTAP apps.\n\n\nUniversal Storage\nSingleStore default table type is its patented Universal Storage table. This combines SingleStore’s three storage layers: memory, disk and object storage. When writing to this table type, data first lands in memory and is immediately available to query. Because data is written to memory first, we achieve extremely high throughput. Data is then flushed to the on-disk columnar store.\n\n\nIn this layer, data is typically compressed by 75-80%, allowing for performant scans of the data whilst pulling out one row via sub-segment access and hash indexes. In addition to this, column group indexes can be used to provide additional performance boosts on OLTP queries. These performance characteristics of our Universal Storage table make it performant in both OLTP and OLAP workloads.\n-_-_-_-_-_-_-_-_\nIn this article, we will guide you through creating this kind of gen AI  app using SingleStore, OpenAI and Next.js. This step-by-step tutorial will help you build and test a micro gen AI app, enabling you to chat with gpt-4o, retrieve random products and render them in custom React components.\n\n\nWe will demonstrate the approach we used to build our gen AI eStore app. This app loads the dashboard in under two seconds by executing five parallel queries that simultaneously read over 100 million rows. It includes a text-to-SQL chat experience over the dashboard data, performs a hybrid (vector + exact keyword match) search to help you find the perfect clothing products for your needs and showcases SingleStore's performance analytics.\n\n\nAdditionally, it performs multiple function calls, all while maintaining efficient and effective performance. The conversational interface also returns agentic widgets that allow users to purchase and rate a product right from the conversational interface.\n\n\nLet’s get started!\n-_-_-_-_-_-_-_-_\n"}],"source":"# Visualize Chunks and Chunks Overlap after the Sentence Splitter Transformation\n\ndocuments = index.docstore.docs\nfor doc in documents.values():\n  print(doc.text)\n  print(\"-_-_-_-_-_-_-_-_\")"},{"cell_type":"code","execution_count":37,"id":"a273ac55-551e-478e-a681-9af3851b1c21","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:20:49.318599Z","iopub.status.busy":"2024-11-21T05:20:49.318183Z","iopub.status.idle":"2024-11-21T05:20:49.415476Z","shell.execute_reply":"2024-11-21T05:20:49.414663Z","shell.execute_reply.started":"2024-11-21T05:20:49.318561Z"},"language":"python","trusted":true},"outputs":[],"source":"# Define a query engine that is responsible for retrieving related pieces of text,\n# and using a LLM to formulate the final answer.\n\nfrom llama_index.llms.gemini import Gemini\n\nllm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=512)\n\nquery_engine = index.as_query_engine(llm=llm)"},{"cell_type":"code","execution_count":38,"id":"e84f771d-a75b-48db-85cb-19e106c08ba7","metadata":{"execution":{"iopub.execute_input":"2024-11-21T05:20:54.018220Z","iopub.status.busy":"2024-11-21T05:20:54.017847Z","iopub.status.idle":"2024-11-21T05:20:55.137812Z","shell.execute_reply":"2024-11-21T05:20:55.127291Z","shell.execute_reply.started":"2024-11-21T05:20:54.018190Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"The npm package name is `@singlestore/client`.\n\n"}],"source":"response = query_engine.query(\"What is SingleStore's npm package name?\")\nprint(response)"},{"cell_type":"code","execution_count":null,"id":"a4d52770-551c-4309-a057-976f9428d91e","metadata":{"language":"python","trusted":true},"outputs":[],"source":""}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"","defaultDatabase":""},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}